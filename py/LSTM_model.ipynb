{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21164155-641f-42b1-8753-55980ef0caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### **Install the Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a21637e6-9b0b-43c6-bfd1-844b70ffc32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\programming\\my_projects\\environments\\dachat\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f5e51e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fb8edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up unused cached memory\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58db7565-9325-42b3-8e07-051197a16c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticker symbol for S&P 500\n",
    "ticker = \"^GSPC\"\n",
    "\n",
    "#yyyy-mm-dd\n",
    "start_date = \"1957-03-04\"\n",
    "end_date = \"2024-09-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a12cbcb-db1d-4a3e-b63f-22569cab946d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "def download_needed_data_to_csv(start_date: str, end_date:str) -> None:\n",
    "    yf.download(ticker, start=start_date, end=end_date, interval='1d').to_csv('../training_data/s&p500.csv')\n",
    "\n",
    "download_needed_data_to_csv(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbdaf68c-6602-42ef-b660-e20d5e05e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv(\"../training_data/s&p500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "712b8faa-1da5-447a-b9fa-63c39fe24803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks = np.array_split(all_data, 15)\n",
    "\n",
    "# # Create a plot\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# for i, chunk in enumerate(chunks):\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "    \n",
    "#     # Assuming you want to plot one column (e.g., 'Close')\n",
    "#     plt.plot(chunk.index, chunk['Close'], label=f'Chunk {i+1}')\n",
    "    \n",
    "#     # Add title and labels\n",
    "#     plt.title(f'Chunk {i+1} of the Data')\n",
    "#     plt.xlabel('Index')\n",
    "#     plt.ylabel('Close Price')\n",
    "#     plt.legend()\n",
    "    \n",
    "#     # Show the plot\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fcd0ca8-a229-48e5-a27f-d3b7846fa470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close        1.000000\n",
      "Adj Close    1.000000\n",
      "Low          0.999969\n",
      "High         0.999964\n",
      "Open         0.999364\n",
      "Volume       0.774932\n",
      "Name: Close, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "all_data_copy = all_data.drop('Date', axis=1)\n",
    "corr_matrix = all_data_copy.corr()\n",
    "print(corr_matrix['Close'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86be8bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Open      High       Low     Close  Adj Close    Volume\n",
      "0  0.007806  0.000902  0.000907  0.000903   0.000903  0.000055\n",
      "1  0.007835  0.000931  0.000936  0.000931   0.000931  0.000052\n",
      "2  0.007837  0.000932  0.000937  0.000933   0.000933  0.000051\n",
      "3  0.007833  0.000929  0.000934  0.000929   0.000929  0.000050\n",
      "4  0.007808  0.000904  0.000909  0.000904   0.000904  0.000032\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize MinMaxScaler to scale data between 0 and 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit and transform the data (excluding the date if necessary)\n",
    "scaled_data = scaler.fit_transform(all_data_copy)\n",
    "\n",
    "# Convert back to DataFrame to visualize (optional)\n",
    "scaled_data = pd.DataFrame(scaled_data, columns=['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'])\n",
    "print(scaled_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55b4217f-c654-4f3a-8f63-7135e2e2a2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mykola\\AppData\\Local\\Temp\\ipykernel_12272\\314545884.py:16: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  X = pd.concat([X, data.loc[i-1:i-1 + seq_length]], ignore_index=True)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mm:\\programming\\my_projects\\environments\\dachat\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m#creating sequences of data\u001b[39;00m\n\u001b[0;32m     45\u001b[0m seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m\n\u001b[1;32m---> 46\u001b[0m x_data, y_data \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(x_data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues))\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(x_data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues))\n",
      "Cell \u001b[1;32mIn[13], line 22\u001b[0m, in \u001b[0;36mcreate_sequences\u001b[1;34m(data, seq_length)\u001b[0m\n\u001b[0;32m     19\u001b[0m         X \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([X, data\u001b[38;5;241m.\u001b[39mloc[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m         y \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([y, data\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(X[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#X, y = clear_tail_from_unused_space(X), clear_tail_from_unused_space(y)\u001b[39;00m\n",
      "File \u001b[1;32mm:\\programming\\my_projects\\environments\\dachat\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mm:\\programming\\my_projects\\environments\\dachat\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from math import ceil\n",
    "\n",
    "def create_sequences(data: pd.DataFrame, seq_length: int):\n",
    "    \"\"\" Returns tuple of needed DataFrames for training and validation\"\"\"\n",
    "    columns = data.columns\n",
    "    X, y = pd.DataFrame(columns=data.columns), pd.DataFrame(columns=data.columns)\n",
    "    print(type(data.iloc[1:3]))\n",
    "    # print(data.head())\n",
    "    # print(data.tail())\n",
    "    \n",
    "    for i in range(1, len(data) - seq_length):\n",
    "        # print(data.loc[i-1:i-1 + seq_length])\n",
    "        # print(data.loc[i-1 + seq_length])\n",
    "        try:\n",
    "            X = pd.concat([X, data.loc[i-1:i-1 + seq_length]], ignore_index=True)\n",
    "            y = pd.concat([y, data.loc[i-1 + seq_length]], ignore_index=True)\n",
    "        except IndexError:\n",
    "            X = pd.concat([X, data.loc[i-1:len(data) - 1]], ignore_index=True)\n",
    "            y = pd.concat([y, data.loc[len(data) - 1]], ignore_index=True)\n",
    "            \n",
    "    print(X[0])\n",
    "    print(X[-1])\n",
    "    \n",
    "    #X, y = clear_tail_from_unused_space(X), clear_tail_from_unused_space(y)\n",
    "    \n",
    "    X, y = pd.DataFrame(X, columns=columns), pd.DataFrame(y, columns=columns)\n",
    "    \n",
    "    X, y = X.drop(['Close', 'Adj Close'], axis=1), y['Close'].copy()\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def clear_tail_from_unused_space(data_array: np.array) -> np.array:\n",
    "    i = len(data_array) - 1\n",
    "    #print(data_array[i])\n",
    "    while np.all(data_array[i] == 0):\n",
    "        i -= 1\n",
    "    # print(data_array[:i])\n",
    "    # print(data_array[:i+1])\n",
    "    return data_array[:i]\n",
    "\n",
    "\n",
    "#creating sequences of data\n",
    "seq_length = 7\n",
    "x_data, y_data = create_sequences(scaled_data, seq_length)\n",
    "\n",
    "print(type(x_data.iloc[:, -1].values))\n",
    "print(type(x_data.iloc[:, 3].values))\n",
    "print(y_data.tail())\n",
    "\n",
    "# Split the dataset: 80% train, 20% validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.20, random_state=42)\n",
    "\n",
    "# print(X_train.tail())\n",
    "# print(X_test.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8bf68d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format datasets for PyTorch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "def format_data_for_torch(data_x: pd.DataFrame, data_y: pd.DataFrame) -> DataLoader:\n",
    "    data_x_tensor = torch.FloatTensor(data_x.values)\n",
    "    data_y_tensor = torch.FloatTensor(data_y.values)\n",
    "    dataset = CustomDataset(data_x_tensor, data_y_tensor)\n",
    "    return DataLoader(dataset=dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "train_loader = format_data_for_torch(X_train, y_train)\n",
    "test_loader = format_data_for_torch(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74c14628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(4, 16, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTMModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Define the output layer (fully connected)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate through the LSTM\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Take the output of the last time step\n",
    "        out = out[:, -1, :]  # Take the last output\n",
    "        \n",
    "        # Pass the last time step output to the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "model = LSTMModel(4, 16, 2, 1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ba3850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimizer and learning rate scheduler\n",
    "num_epochs = 64\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "learning_rate = 0.1\n",
    "\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9eec9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load accuracy metric\n",
    "from evaluate import load\n",
    "metric = load('accuracy')\n",
    "\n",
    "# Initialize lists to store loss and accuracy\n",
    "avg_train_losses = []\n",
    "avg_val_losses = []\n",
    "validation_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67b8f996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss for epoch 1: 0.041738062305594556\n",
      "Average validation loss for epoch 1: 0.03938803754539953\n",
      "Average training loss for epoch 2: 0.038782117582262965\n",
      "Average validation loss for epoch 2: 0.03730627754703164\n",
      "Average training loss for epoch 3: 0.036589854606755186\n",
      "Average validation loss for epoch 3: 0.03441309063108983\n",
      "Average training loss for epoch 4: 0.03246757507044385\n",
      "Average validation loss for epoch 4: 0.029103083518782148\n",
      "Average training loss for epoch 5: 0.024321941173517367\n",
      "Average validation loss for epoch 5: 0.01808148660455589\n",
      "Average training loss for epoch 6: 0.011471670806329225\n",
      "Average validation loss for epoch 6: 0.005495335242745501\n",
      "Average training loss for epoch 7: 0.00288779040245982\n",
      "Average validation loss for epoch 7: 0.0015924644723310376\n",
      "Average training loss for epoch 8: 0.0012262876149230885\n",
      "Average validation loss for epoch 8: 0.0011368857635210992\n",
      "Average training loss for epoch 9: 0.000965843398010297\n",
      "Average validation loss for epoch 9: 0.0008918389907598288\n",
      "Average training loss for epoch 10: 0.0008066327935806148\n",
      "Average validation loss for epoch 10: 0.0007433754602701541\n",
      "Average training loss for epoch 11: 0.0006766566695719782\n",
      "Average validation loss for epoch 11: 0.0006177823859750923\n",
      "Average training loss for epoch 12: 0.0005699781684708856\n",
      "Average validation loss for epoch 12: 0.0005273264099809307\n",
      "Average training loss for epoch 13: 0.0004837143934766889\n",
      "Average validation loss for epoch 13: 0.00044994075115364804\n",
      "Average training loss for epoch 14: 0.00041318219411268854\n",
      "Average validation loss for epoch 14: 0.0003828330314313967\n",
      "Average training loss for epoch 15: 0.0003565429781791492\n",
      "Average validation loss for epoch 15: 0.00034705531585064753\n",
      "Average training loss for epoch 16: 0.00030874486196487747\n",
      "Average validation loss for epoch 16: 0.0002908182802827839\n",
      "Average training loss for epoch 17: 0.00027186968171162345\n",
      "Average validation loss for epoch 17: 0.0002669647835352441\n",
      "Average training loss for epoch 18: 0.00024016324029399187\n",
      "Average validation loss for epoch 18: 0.000227727766927004\n",
      "Average training loss for epoch 19: 0.00021380095298843623\n",
      "Average validation loss for epoch 19: 0.00020595265110993446\n",
      "Average training loss for epoch 20: 0.00019269965969855753\n",
      "Average validation loss for epoch 20: 0.00019685590666119457\n",
      "Average training loss for epoch 21: 0.0001748162306716202\n",
      "Average validation loss for epoch 21: 0.00016956561524205394\n",
      "Average training loss for epoch 22: 0.0001598078001200652\n",
      "Average validation loss for epoch 22: 0.00015600872838048317\n",
      "Average training loss for epoch 23: 0.00014716416477876662\n",
      "Average validation loss for epoch 23: 0.00014464477802296946\n",
      "Average training loss for epoch 24: 0.00013617359313834021\n",
      "Average validation loss for epoch 24: 0.00013407359289919474\n",
      "Average training loss for epoch 25: 0.00012703211831875547\n",
      "Average validation loss for epoch 25: 0.00012505684720020294\n",
      "Average training loss for epoch 26: 0.00011886785106357415\n",
      "Average validation loss for epoch 26: 0.00011829092074077818\n",
      "Average training loss for epoch 27: 0.00011195625490938015\n",
      "Average validation loss for epoch 27: 0.00011478491102968326\n",
      "Average training loss for epoch 28: 0.00010569260746091408\n",
      "Average validation loss for epoch 28: 0.00010469265267871649\n",
      "Average training loss for epoch 29: 0.0001000728143694452\n",
      "Average validation loss for epoch 29: 9.960574944098307e-05\n",
      "Average training loss for epoch 30: 9.50377306565063e-05\n",
      "Average validation loss for epoch 30: 9.712122253538971e-05\n",
      "Average training loss for epoch 31: 9.087969794538213e-05\n",
      "Average validation loss for epoch 31: 8.97871186343427e-05\n",
      "Average training loss for epoch 32: 8.687039363894014e-05\n",
      "Average validation loss for epoch 32: 8.657087905535095e-05\n",
      "Average training loss for epoch 33: 8.29698026603571e-05\n",
      "Average validation loss for epoch 33: 8.302789056971151e-05\n",
      "Average training loss for epoch 34: 7.990573564237705e-05\n",
      "Average validation loss for epoch 34: 7.917007328446575e-05\n",
      "Average training loss for epoch 35: 7.678080713480247e-05\n",
      "Average validation loss for epoch 35: 7.838980322789953e-05\n",
      "Average training loss for epoch 36: 7.392369573014928e-05\n",
      "Average validation loss for epoch 36: 7.453396736107404e-05\n",
      "Average training loss for epoch 37: 7.10952710437489e-05\n",
      "Average validation loss for epoch 37: 7.098698956219273e-05\n",
      "Average training loss for epoch 38: 6.87150997794266e-05\n",
      "Average validation loss for epoch 38: 6.926090558139279e-05\n",
      "Average training loss for epoch 39: 6.666159784167725e-05\n",
      "Average validation loss for epoch 39: 6.828028435174255e-05\n",
      "Average training loss for epoch 40: 6.451676910873744e-05\n",
      "Average validation loss for epoch 40: 6.557947974080755e-05\n",
      "Average training loss for epoch 41: 6.255639628578248e-05\n",
      "Average validation loss for epoch 41: 6.335186743753083e-05\n",
      "Average training loss for epoch 42: 6.0789637130473634e-05\n",
      "Average validation loss for epoch 42: 6.082971369189046e-05\n",
      "Average training loss for epoch 43: 5.902780197094033e-05\n",
      "Average validation loss for epoch 43: 6.0271049733273685e-05\n",
      "Average training loss for epoch 44: 5.753011973524202e-05\n",
      "Average validation loss for epoch 44: 5.7677938625770536e-05\n",
      "Average training loss for epoch 45: 5.6092394618350276e-05\n",
      "Average validation loss for epoch 45: 5.658767339708801e-05\n",
      "Average training loss for epoch 46: 5.4624544534757906e-05\n",
      "Average validation loss for epoch 46: 5.589284939088867e-05\n",
      "Average training loss for epoch 47: 5.328059571713456e-05\n",
      "Average validation loss for epoch 47: 5.42059486313654e-05\n",
      "Average training loss for epoch 48: 5.208727912852997e-05\n",
      "Average validation loss for epoch 48: 5.209481619035446e-05\n",
      "Average training loss for epoch 49: 5.0940157116585886e-05\n",
      "Average validation loss for epoch 49: 5.161678661722733e-05\n",
      "Average training loss for epoch 50: 4.979624043724624e-05\n",
      "Average validation loss for epoch 50: 5.107320453977014e-05\n",
      "Average training loss for epoch 51: 4.87074832281124e-05\n",
      "Average validation loss for epoch 51: 4.91718952886273e-05\n",
      "Average training loss for epoch 52: 4.795779587454742e-05\n",
      "Average validation loss for epoch 52: 4.7854335763178016e-05\n",
      "Average training loss for epoch 53: 4.7050066608738834e-05\n",
      "Average validation loss for epoch 53: 4.772890157188091e-05\n",
      "Average training loss for epoch 54: 4.613247040143792e-05\n",
      "Average validation loss for epoch 54: 4.674218113602061e-05\n",
      "Average training loss for epoch 55: 4.5326754963257735e-05\n",
      "Average validation loss for epoch 55: 4.5594211196994705e-05\n",
      "Average training loss for epoch 56: 4.457326055880955e-05\n",
      "Average validation loss for epoch 56: 4.5405399692686774e-05\n",
      "Average training loss for epoch 57: 4.3912362031985136e-05\n",
      "Average validation loss for epoch 57: 4.389981264736249e-05\n",
      "Average training loss for epoch 58: 4.329493364560781e-05\n",
      "Average validation loss for epoch 58: 4.3417549561666046e-05\n",
      "Average training loss for epoch 59: 4.262444373283605e-05\n",
      "Average validation loss for epoch 59: 4.299783749620137e-05\n",
      "Average training loss for epoch 60: 4.205032463017152e-05\n",
      "Average validation loss for epoch 60: 4.31890320113679e-05\n",
      "Average training loss for epoch 61: 4.140922640079464e-05\n",
      "Average validation loss for epoch 61: 4.227387974727511e-05\n",
      "Average training loss for epoch 62: 4.090034145037392e-05\n",
      "Average validation loss for epoch 62: 4.089858472340253e-05\n",
      "Average training loss for epoch 63: 4.0425807815944196e-05\n",
      "Average validation loss for epoch 63: 4.104113129330842e-05\n",
      "Average training loss for epoch 64: 3.992692157699936e-05\n",
      "Average validation loss for epoch 64: 4.023492372787722e-05\n"
     ]
    }
   ],
   "source": [
    "#train and validation\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    for batch_id, (x, y) in enumerate(train_loader):\n",
    "        inputs, targets = x.unsqueeze(1).to(device), y.float().unsqueeze(1).to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "    avg_train_losses.append(avg_train_loss)\n",
    "    print(f'Average training loss for epoch {epoch+1}: {avg_train_loss}')\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    var_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            inputs, targets = x.unsqueeze(1).to(device), y.float().unsqueeze(1).to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            test_loss = loss_function(outputs, targets)\n",
    "            \n",
    "            var_losses.append(test_loss.item())\n",
    "        \n",
    "        avg_val_loss = sum(var_losses) / len(var_losses)\n",
    "        avg_val_losses.append(avg_val_loss)\n",
    "        print(f'Average validation loss for epoch {epoch+1}: {avg_val_loss}')\n",
    "\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'lstm_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03745e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Open         High          Low     Volume\n",
      "Date                                                       \n",
      "2024-09-30  5726.52002  5743.859863  5724.350098  992236769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 8\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_pred)\n",
      "File \u001b[1;32mm:\\programming\\my_projects\\environments\\dachat\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mm:\\programming\\my_projects\\environments\\dachat\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 15\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Initialize hidden and cell states\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     16\u001b[0m     c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Forward propagate through the LSTM\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "test_data = yf.download(ticker, start='2024-09-30', end='2024-10-01', interval='1d')\n",
    "#test_data = test_data.drop('Date')\n",
    "test_data_x = test_data.drop(['Close', 'Adj Close'], axis=1)\n",
    "print(test_data_x)\n",
    "label= test_data['Close'].copy()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_data_x)\n",
    "y_pred = y_pred.cpu().numpy()\n",
    "print(y_pred)\n",
    "print(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dachat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
